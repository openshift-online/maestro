name: Maestro Long Running Test

on:
  schedule:
  - cron: '0 2 * * 1-5'  # run every work day

  # Allow manual triggering of the workflow
  workflow_dispatch:

jobs:
  longrunning-cluster-e2e:
    runs-on: ubuntu-latest
    permissions:
      id-token: 'write'
      contents: 'read'

    steps:
      - name: Checkout repo
        uses: actions/checkout@v4

      # Login to Azure using OIDC (no secret needed if federated identity is set up)
      - name: Azure Login
        uses: azure/login@v2
        with:
          client-id: ${{ secrets.AZURE_CLIENT_ID }}
          tenant-id: ${{ secrets.AZURE_TENANT_ID }}
          subscription-id: ${{ secrets.AZURE_SUBSCRIPTION_ID }}
          # if not using federated identity:
          # client-secret: ${{ secrets.AZURE_CLIENT_SECRET }}

      # Install kubelogin (needed for AAD-enabled clusters)
      - name: Install kubelogin and jq
        run: |
          az aks install-cli
          kubelogin --version
          sudo apt install -y jq gettext

      # Get AKS credentials
      - name: Get AKS credentials
        id: get_credentials
        run: |
          az aks get-credentials \
            --resource-group ${{ secrets.SVC_RESOURCE_GROUP }} \
            --name ${{ secrets.SVC_CLUSTER_NAME }} \
            --overwrite-existing \
            -f ./svc-cluster.kubeconfig
          az aks get-credentials \
            --resource-group ${{ secrets.MGMT_RESOURCE_GROUP }} \
            --name ${{ secrets.MGMT_CLUSTER_NAME }} \
            --overwrite-existing \
            -f ./mgmt-cluster.kubeconfig

      # Send Slack notification if credentials retrieval failed
      - name: Notify Slack on credentials failure
        uses: slackapi/slack-github-action@v2.1.1
        if: failure() && steps.get_credentials.outcome == 'failure'
        with:
          webhook: ${{ secrets.SLACK_WEBHOOK_URL }}
          webhook-type: incoming-webhook
          payload: |
            text: ":x: *AKS Credentials Retrieval Failed* - Cluster may be down! <!subteam^S09NY6A9QH3>\n"
            blocks:
              - type: "section"
                text:
                  type: "mrkdwn"
                  text: ":x: *AKS Credentials Retrieval Failed*\n\n<!subteam^S09NY6A9QH3> The long-running cluster may be down or inaccessible.\n\n*Clusters:*\n- SVC Cluster: ${{ secrets.SVC_CLUSTER_NAME }}\n- MGMT Cluster: ${{ secrets.MGMT_CLUSTER_NAME }}\n\n<${{ github.server_url }}/${{ github.repository }}/actions/runs/${{ github.run_id }}|View logs>"

      # Convert kubeconfig for non-interactive login (GitHub runner)
      - name: Convert kubeconfig for kubelogin
        run: |
          kubelogin convert-kubeconfig -l azurecli --kubeconfig ./svc-cluster.kubeconfig
          kubelogin convert-kubeconfig -l azurecli --kubeconfig ./mgmt-cluster.kubeconfig

      # Verify kubectl works
      - name: Check cluster access
        run: |
          kubectl --kubeconfig ./svc-cluster.kubeconfig get pods -A -l app=maestro
          kubectl --kubeconfig ./svc-cluster.kubeconfig get service -n maestro
          kubectl --kubeconfig ./mgmt-cluster.kubeconfig get pods -A -l app=maestro-agent

      - name: generate the in cluster kubeconfig
        run: |
          generate_in_cluster_kube() {
            kubeconfig=$1
            type=$2
            kubectl --kubeconfig "$kubeconfig" -n default create serviceaccount e2e-test-admin

            cat << -EOF | kubectl --kubeconfig "$kubeconfig" apply -f -
          apiVersion: rbac.authorization.k8s.io/v1
          kind: ClusterRoleBinding
          metadata:
            name: e2e-test-admin
          roleRef:
            apiGroup: rbac.authorization.k8s.io
            kind: ClusterRole
            name: cluster-admin
          subjects:
          - kind: ServiceAccount
            name: e2e-test-admin
            namespace: default
          -EOF

            TOKEN=$(kubectl --kubeconfig "$kubeconfig" create token e2e-test-admin --namespace default --duration=8760h)
            API_SERVER=$(kubectl --kubeconfig "$kubeconfig" config view -o jsonpath='{.clusters[0].cluster.server}')
            CA_CERT=$(kubectl --kubeconfig "$kubeconfig" config view --raw -o jsonpath='{.clusters[0].cluster.certificate-authority-data}')

            cat > "${type}-incluster.kubeconfig" << -EOF
          apiVersion: v1
          kind: Config
          clusters:
          - name: my-cluster
            cluster:
              server: "$API_SERVER"
              certificate-authority-data: "$CA_CERT"
          users:
          - name: e2e-test-admin
            user:
              token: "$TOKEN"
          contexts:
          - name: e2e-test-admin-context
            context:
              cluster: my-cluster
              user: e2e-test-admin
              namespace: default
          current-context: e2e-test-admin-context
          -EOF
          }

          generate_in_cluster_kube $(pwd)/svc-cluster.kubeconfig svc
          generate_in_cluster_kube $(pwd)/mgmt-cluster.kubeconfig mgmt

      - name: start pre-upgrade
        id: pre_upgrade
        continue-on-error: true
        run: |
          pod_template_hash=$(kubectl --kubeconfig "$(pwd)/svc-cluster.kubeconfig" get rs -l app=maestro -n maestro -o jsonpath='{range .items[?(@.spec.replicas>0)]}{.metadata.name}{"\n"}{end}' | awk -F '-' '{print $2}')
          if [ -z "$pod_template_hash" ]; then
            echo "Error: No active replicaset found"
            exit 1
          fi
          pod_name=$(kubectl --kubeconfig "$(pwd)/svc-cluster.kubeconfig" get pods -n maestro -l pod-template-hash=$pod_template_hash -o jsonpath='{.items[0].metadata.name}')
          if [ -z "$pod_name" ]; then
            echo "Error: No pod found for replicaset hash $pod_template_hash"
            exit 1
          fi
          commit_sha=$(kubectl --kubeconfig "$(pwd)/svc-cluster.kubeconfig" logs -n maestro $pod_name | grep -i "Git Commit" | grep -oE '[a-f0-9]{40}')
          if [ -z "$commit_sha" ]; then
            echo "Error: Could not extract commit SHA from pod logs"
            exit 1
          fi
          consumer_name=$(kubectl --kubeconfig "$(pwd)/mgmt-cluster.kubeconfig" get deployment maestro-agent -n maestro -o yaml | grep -E "^\s+- --consumer-name=" | sed 's/.*--consumer-name=//'| head -1)
          if [ -z "$consumer_name" ]; then
            echo "Error: Could not extract consumer name from agent deployment"
            exit 1
          fi

          echo "image_sha=$commit_sha" >> $GITHUB_OUTPUT
          echo "consumer_name=$consumer_name" >> $GITHUB_OUTPUT

          IMAGE=quay.io/redhat-user-workloads/maestro-rhtap-tenant/maestro-e2e:$commit_sha \
            CONSUMER_NAME=$consumer_name \
            SERVER_KUBECONFIG=$(pwd)/svc-cluster.kubeconfig \
            AGENT_IN_CLUSTER_KUBECONFIG=$(pwd)/mgmt-incluster.kubeconfig \
            SERVICE_ACCOUNT_NAME=clusters-service \
            ENABLE_AUTHORIZATION_POLICY=true \
            bash -x test/upgrade/script/run.sh || exit 1

      - name: start to upgrade server
        id: upgrade_server
        continue-on-error: true
        if: steps.pre_upgrade.outcome == 'success'
        run: |
          KUBECONFIG=$(pwd)/svc-cluster.kubeconfig make e2e/rollout
          consumer_name="${{ steps.pre_upgrade.outputs.consumer_name }}"
          commit_sha="${{ steps.pre_upgrade.outputs.image_sha }}"

          AGENT_NAMESPACE=maestro \
            IMAGE=quay.io/redhat-user-workloads/maestro-rhtap-tenant/maestro-e2e:$commit_sha \
            CONSUMER_NAME=$consumer_name \
            SERVER_KUBECONFIG=$(pwd)/svc-cluster.kubeconfig \
            AGENT_KUBECONFIG=$(pwd)/mgmt-cluster.kubeconfig \
            SERVER_IN_CLUSTER_KUBECONFIG=$(pwd)/svc-incluster.kubeconfig \
            AGENT_IN_CLUSTER_KUBECONFIG=$(pwd)/mgmt-incluster.kubeconfig \
            SERVICE_ACCOUNT_NAME=clusters-service \
            bash -x test/e2e/istio/test.sh || exit 1

          IMAGE=quay.io/redhat-user-workloads/maestro-rhtap-tenant/maestro-e2e:$commit_sha \
            CONSUMER_NAME=$consumer_name \
            SERVER_KUBECONFIG=$(pwd)/svc-cluster.kubeconfig \
            AGENT_IN_CLUSTER_KUBECONFIG=$(pwd)/mgmt-incluster.kubeconfig \
            SERVICE_ACCOUNT_NAME=clusters-service \
            ENABLE_AUTHORIZATION_POLICY=true \
            bash -x test/upgrade/script/run.sh || exit 1

      - name: start to upgrade agent
        id: upgrade_agent
        continue-on-error: true
        if: steps.pre_upgrade.outcome == 'success'
        run: |
          KUBECONFIG=$(pwd)/mgmt-cluster.kubeconfig make e2e/rollout
          consumer_name="${{ steps.pre_upgrade.outputs.consumer_name }}"
          pod_template_hash=$(kubectl --kubeconfig "$(pwd)/svc-cluster.kubeconfig" get rs -l app=maestro -n maestro -o jsonpath='{range .items[?(@.spec.replicas>0)]}{.metadata.name}{"\n"}{end}' | awk -F '-' '{print $2}')
          if [ -z "$pod_template_hash" ]; then
            echo "Error: No active replicaset found"
            exit 1
          fi
          pod_name=$(kubectl --kubeconfig "$(pwd)/svc-cluster.kubeconfig" get pods -n maestro -l pod-template-hash=$pod_template_hash -o jsonpath='{.items[0].metadata.name}')
          if [ -z "$pod_name" ]; then
            echo "Error: No pod found for replicaset hash $pod_template_hash"
            exit 1
          fi
          commit_sha=$(kubectl --kubeconfig "$(pwd)/svc-cluster.kubeconfig" logs -n maestro $pod_name | grep -i "Git Commit" | grep -oE '[a-f0-9]{40}')
          if [ -z "$commit_sha" ]; then
            echo "Error: Could not extract commit SHA from pod logs"
            exit 1
          fi

          AGENT_NAMESPACE=maestro \
            IMAGE=quay.io/redhat-user-workloads/maestro-rhtap-tenant/maestro-e2e:$commit_sha \
            CONSUMER_NAME=$consumer_name \
            SERVER_KUBECONFIG=$(pwd)/svc-cluster.kubeconfig \
            AGENT_KUBECONFIG=$(pwd)/mgmt-cluster.kubeconfig \
            SERVER_IN_CLUSTER_KUBECONFIG=$(pwd)/svc-incluster.kubeconfig \
            AGENT_IN_CLUSTER_KUBECONFIG=$(pwd)/mgmt-incluster.kubeconfig \
            SERVICE_ACCOUNT_NAME=clusters-service \
            bash -x test/e2e/istio/test.sh || exit 1

          IMAGE=quay.io/redhat-user-workloads/maestro-rhtap-tenant/maestro-e2e:$commit_sha \
            CONSUMER_NAME=$consumer_name \
            SERVER_KUBECONFIG=$(pwd)/svc-cluster.kubeconfig \
            AGENT_IN_CLUSTER_KUBECONFIG=$(pwd)/mgmt-incluster.kubeconfig \
            SERVICE_ACCOUNT_NAME=clusters-service \
            ENABLE_AUTHORIZATION_POLICY=true \
            bash -x test/upgrade/script/run.sh || exit 1

      - name: Post a message in a channel
        uses: slackapi/slack-github-action@v2.1.1
        if: always()
        with:
          webhook: ${{ secrets.SLACK_WEBHOOK_URL }}
          webhook-type: incoming-webhook
          payload: |
            text: "${{ job.status == 'success' && ':white_check_mark:' || ':x:' }} *Maestro Long Running E2E Test*: ${{ job.status }}${{ job.status != 'success' && ' <!subteam^S09NY6A9QH3>' || '' }}\n"
            blocks:
              - type: "section"
                text:
                  type: "mrkdwn"
                  text: "${{ job.status == 'success' && ':white_check_mark:' || ':x:' }} *Maestro Long Running E2E Test Result: ${{ job.status }}*${{ job.status != 'success' && '\n\n<!subteam^S09NY6A9QH3> Please check the failed test cases below:' || '' }}\n\n*Test Steps Status:*\n${{ steps.pre_upgrade.outcome == 'success' && ':white_check_mark:' || steps.pre_upgrade.outcome == 'failure' && ':x:' || ':heavy_minus_sign:' }} Pre-upgrade tests\n${{ steps.upgrade_server.outcome == 'success' && ':white_check_mark:' || steps.upgrade_server.outcome == 'failure' && ':x:' || ':heavy_minus_sign:' }} Upgrade server tests\n${{ steps.upgrade_agent.outcome == 'success' && ':white_check_mark:' || steps.upgrade_agent.outcome == 'failure' && ':x:' || ':heavy_minus_sign:' }} Upgrade agent tests\n\n${{ (steps.pre_upgrade.outcome == 'failure' || steps.upgrade_server.outcome == 'failure' || steps.upgrade_agent.outcome == 'failure') && '*Failed Test Cases:*\n' || '' }}${{ steps.pre_upgrade.outcome == 'failure' && '- Pre-upgrade test suite failed\n' || '' }}${{ steps.upgrade_server.outcome == 'failure' && '- Server upgrade test suite failed\n' || '' }}${{ steps.upgrade_agent.outcome == 'failure' && '- Agent upgrade test suite failed\n' || '' }}\n<${{ github.server_url }}/${{ github.repository }}/actions/runs/${{ github.run_id }}|View detailed logs>"

      - name: Clean up
        if: always()
        run: |
          kubectl --kubeconfig $(pwd)/svc-cluster.kubeconfig delete serviceaccount e2e-test-admin
          kubectl --kubeconfig $(pwd)/svc-cluster.kubeconfig delete clusterrolebinding e2e-test-admin
          kubectl --kubeconfig $(pwd)/mgmt-cluster.kubeconfig delete serviceaccount e2e-test-admin
          kubectl --kubeconfig $(pwd)/mgmt-cluster.kubeconfig delete clusterrolebinding e2e-test-admin
